{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"!pip install python-dateutil --upgrade\n!pip install awscli --upgrade\n!pip install greenlet --ignore-installed --upgrade\n!pip install allennlp","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n\nFOLD = 0\n\nimport os\nimport sys\nimport random\nimport glob\nimport gc\nimport logging\nimport requests\nimport re\n\nfrom typing import Dict, Tuple, List\nfrom collections import OrderedDict\nfrom overrides import overrides\nfrom time import sleep\n\nimport cv2\nimport numpy as np\nimport pandas as pd\n\nimport mlcrate as mlc\n\nfrom sklearn.model_selection import KFold\n\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch import optim\n\nimport torchvision\n\nimport allennlp\n\nfrom allennlp.common import Registrable, Params\nfrom allennlp.common.util import START_SYMBOL, END_SYMBOL, JsonDict\n\nfrom allennlp.data import DatasetReader, Instance\nfrom allennlp.data.fields import ArrayField, TextField\nfrom allennlp.data.iterators import BucketIterator, MultiprocessIterator\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer\nfrom allennlp.data.tokenizers import Token, CharacterTokenizer\nfrom allennlp.data.vocabulary import Vocabulary\n\nfrom allennlp.models import Model\n\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper # MIGHT USE FOR ABSTRACTION\n\nfrom allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\nfrom allennlp.nn.beam_search import BeamSearch\n\nfrom allennlp.training.metrics import F1Measure, BLEU\nfrom allennlp.training import Trainer\n\nsys.path.insert(0, './math_handwriting_recognition')\n\nlogger = logging.getLogger()\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# device = torch.device(\"cpu\")\n\nKEY = 'iDCO9Ns00jOTY_Db3KuaVLhjux-HKPp_tEtV8LEtesP'\n\ndef notify(value_1, value_2, value_3='', key=KEY):\n    report = {}\n    report['value1'] = value_1\n    report['value2'] = value_2\n    report['value3'] = value_3\n\n    requests.post(f'https://maker.ifttt.com/trigger/notification/with/key/{key}', data=report)\n    requests.post(f'https://maker.ifttt.com/trigger/email/with/key/{key}', data=report)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f3e3fdb35f020dd633693df14fea801fd4d240b"},"cell_type":"code","source":"!mkdir logs\n!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n!unzip -o ngrok-stable-linux-amd64.zip\nLOG_DIR = './logs' # Here you have to put your log directory\nget_ipython().system_raw(\n    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n    .format(LOG_DIR)\n)\nget_ipython().system_raw('./ngrok http 6006 &')\n\n!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n        \ntemp = !curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys,json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\nprint(temp[0])\n\nnotify('Tensorboard', f'Tensorboard at: {temp[0]}', temp[0])\n\n!rm ngrok\n!rm ngrok-stable-linux-amd64.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e6d4291d913e9eb45b259d401074243fa5bb077"},"cell_type":"code","source":"# #Alternative to ngrok\n# !mkdir logs\n# get_ipython().system_raw('tensorboard --logdir ./logs --host 0.0.0.0 --port 6006 &')\n# !ssh -o \"StrictHostKeyChecking no\" -R 80:localhost:6006 serveo.net","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18c3ee01a16d1a3cffef4464eec81e6ef405c12f"},"cell_type":"code","source":"!tar -xf ../input/crohme-2019-unofficial-processed/train.tgz -C ./\n!tar -xf ../input/crohme-2019-unofficial-processed/val.tgz -C ./\n\n!mkdir math_handwriting_recognition\n!touch math_handwriting_recognition/__init__.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('./crohme-train/train.csv')\nkfold = KFold(n_splits=10, shuffle=True, random_state=1337)\ntrain_idx, val_idx = list(kfold.split(train_df))[0]\ntrain_df, val_df = train_df.iloc[train_idx].reset_index(), train_df.iloc[val_idx].reset_index()\ntrain_df.to_csv('./crohme-train/train.csv')\nval_df.to_csv('./crohme-train/val.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53fe6dcbfe7562e408068a6669d21b838b00d3d8"},"cell_type":"code","source":"%%writefile math_handwriting_recognition/dataset.py\nimport os\nimport random\nfrom typing import Dict, Tuple, List\nfrom overrides import overrides\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\n\nimport spacy\n\nimport allennlp\n\nfrom allennlp.common.util import START_SYMBOL, END_SYMBOL, get_spacy_model\n\nfrom allennlp.data import DatasetReader, Instance\nfrom allennlp.data.fields import ArrayField, TextField, MetadataField\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer\nfrom allennlp.data.tokenizers import Token, Tokenizer, CharacterTokenizer, WordTokenizer\n\n@Tokenizer.register(\"latex\")\nclass LatexTokenizer(Tokenizer):\n    def __init__(self) -> None:\n        super().__init__()\n\n    def _tokenize(self, text):        \n        text = text.replace('(', ' ( ')\n        text = text.replace(')', ' ) ')\n        text = text.replace('{', ' { ')\n        text = text.replace('}', ' } ')\n#         text = text.replace('$', ' $ ')\n        text = text.replace('$', '')\n        text = text.replace('_', ' _ ')\n        text = text.replace('^', ' ^ ')\n        text = text.replace('+', ' + ')\n        text = text.replace('-', ' - ')\n        text = text.replace('/', ' / ')\n        text = text.replace('*', ' * ')\n        text = text.replace('=', ' = ')\n        text = text.replace('[', ' [ ')\n        text = text.replace(']', ' ] ')\n        text = text.replace('|', ' | ')\n        text = text.replace('!', ' ! ')\n        text = text.replace(',', ' , ')\n        \n        text = text.replace('\\\\', ' \\\\')\n        \n        text = text.replace('0', ' 0 ')\n        text = text.replace('1', ' 1 ')\n        text = text.replace('2', ' 2 ')\n        text = text.replace('3', ' 3 ')\n        text = text.replace('4', ' 4 ')\n        text = text.replace('5', ' 5 ')\n        text = text.replace('6', ' 6 ')\n        text = text.replace('7', ' 7 ')\n        text = text.replace('8', ' 8 ')\n        text = text.replace('9', ' 9 ')\n        \n        text2 = ''\n        for word in text.split():\n            if len(word) > 1:\n                if word[0] != '\\\\':\n                    for char in word:\n                        text2 += f' {char}'\n                else:\n                    text2 += f' {word}'\n            else:\n                text2 += f' {word}'\n\n        return [Token(token) for token in text2.split()]\n\n    @overrides\n    def tokenize(self, text: str) -> List[Token]:\n        tokens = self._tokenize(text)\n\n        return tokens\n    \n# From https://jdhao.github.io/2017/11/06/resize-image-to-square-with-padding/\ndef resize(im, desired_size):\n\n    old_size = im.shape[:2] # old_size is in (height, width) format\n\n    ratio = float(desired_size)/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n\n    # new_size should be in (width, height) format\n\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n\n    delta_w = desired_size - new_size[1]\n    delta_h = desired_size - new_size[0]\n    top, bottom = delta_h//2, delta_h-(delta_h//2)\n    left, right = delta_w//2, delta_w-(delta_w//2)\n\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,\n        value=color)\n\n    return new_im\n\n@DatasetReader.register('CROHME')\nclass CROHMEDatasetReader(DatasetReader):\n    def __init__(self, root_path: str, tokenizer: Tokenizer, height: int = 512, width: int = 512, lazy: bool = True,\n                 subset: bool = False) -> None:\n        super().__init__(lazy)\n        \n        self.mean = 0.4023\n        self.std = 0.4864\n        \n        self.root_path = root_path\n        self.height = height\n        self.width = width\n        self.subset = subset\n        \n        self._tokenizer = tokenizer\n        self._token_indexer = {\"tokens\": SingleIdTokenIndexer()}\n\n    @overrides\n    def _read(self, file: str):\n        df = pd.read_csv(os.path.join(self.root_path, file))\n        if self.subset:\n            df = df.loc[:16]\n\n        for _, row in df.iterrows():\n            img_id = row['id']\n            \n            if 'label' in df.columns:\n                label = row['label']\n                yield self.text_to_instance(file, img_id, label)\n            else:\n                yield self.text_to_instance(file, img_id)\n            \n    @overrides\n    def text_to_instance(self, file: str, img_id: int, label: str = None) -> Instance:\n        sub_path = file.split('/')[0]\n        path = os.path.join(self.root_path, sub_path, 'data', f'{img_id}.png')\n\n        img = (1 - plt.imread(path)[:,:,0])\n        img = img.reshape(1, img.shape[0], img.shape[1])\n        img = np.concatenate((img, img, img))\n        img = cv2.resize(img.transpose(1, 2, 0), (self.width, self.height)).transpose(2, 0, 1)\n        img = np.rint(img)\n    \n        fields = {}\n        fields['metadata'] = MetadataField({'path': path})\n        fields['img'] = ArrayField(img)\n        \n        if label is not None:\n            label = self._tokenizer.tokenize(label)\n\n            label.insert(0, Token(START_SYMBOL))\n            label.append(Token(END_SYMBOL))\n            \n            fields['label'] = TextField(label, self._token_indexer)\n        \n        return Instance(fields)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b073aaeda0eb82a0d9fd840499a53c049a9c47cf"},"cell_type":"code","source":"%%writefile math_handwriting_recognition/metrics.py\nimport os\nimport random\nimport subprocess\nfrom typing import Dict, Tuple\nfrom overrides import overrides\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport allennlp\n\nfrom allennlp.common import Registrable, Params\nfrom allennlp.common.util import START_SYMBOL, END_SYMBOL\n\nfrom allennlp.training.metrics import Metric, F1Measure, BLEU, BooleanAccuracy\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# From https://github.com/allenai/allennlp/blob/master/allennlp/training/metrics/boolean_accuracy.py\n@Metric.register(\"exprate\")\nclass Exprate(Metric):\n    def __init__(self, end_index: int, vocab) -> None:\n        self._correct = 0.0\n        self._total = 0.0\n        self.vocab = vocab\n        \n        self._end_index = end_index\n\n    def __call__(self, predictions: torch.Tensor, targets: torch.Tensor):\n        predictions, targets = self.unwrap_to_tensors(predictions, targets)\n        batch_size = predictions.size(0)\n\n        # Shape: (batch_size, -1)\n        predictions = predictions.view(batch_size, -1)\n        # Shape: (batch_size, -1)\n        targets = targets.view(batch_size, -1)\n\n        # Get index of eos token in targets\n        end_indices = (targets == self._end_index).nonzero()[:, 1]\n        \n        # Check if each prediction in batch is identical to target\n        for i in range(batch_size):\n            end_index = end_indices[i]\n            \n            # Shape: (1, -1)\n            target = targets[i, :end_index]\n\n            # Shape: (1, -1)\n            prediction = predictions[i, :end_index]\n            \n            if torch.equal(prediction, target):\n                self._correct += 1\n            self._total += 1\n\n    def get_metric(self, reset: bool = False):\n        accuracy = float(self._correct) / float(self._total)\n        if reset:\n            self.reset()\n        return {'exprate': accuracy}\n\n    @overrides\n    def reset(self):\n        self._correct = 0.0\n        self._total = 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a5504db81736fc39bfc237ffc81bb2a6cbb3065"},"cell_type":"code","source":"%%writefile math_handwriting_recognition/encoder.py\nimport os\nimport random\nfrom typing import Dict, Tuple, List\nfrom overrides import overrides\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchvision\n\nimport allennlp\n\nfrom allennlp.common import Registrable, Params\n\nfrom allennlp.data import Vocabulary\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65939\nclass CSE(nn.Module):\n    def __init__(self, in_ch, r):\n        super(CSE, self).__init__()\n        \n        self.linear_1 = nn.Linear(in_ch, in_ch//r)\n        self.linear_2 = nn.Linear(in_ch//r, in_ch)\n    \n    def forward(self, x):\n        input_x = x\n\n        x = x.view(*(x.shape[:-2]),-1).mean(-1)\n        x = F.relu(self.linear_1(x), inplace=True)\n        x = self.linear_2(x)\n        x = x.unsqueeze(-1).unsqueeze(-1)\n        x = torch.sigmoid(x)\n\n        x = torch.mul(input_x, x)\n        \n        return x\n\nclass SSE(nn.Module):\n    def __init__(self, in_ch):\n        super(SSE, self).__init__()\n        \n        self.conv = nn.Conv2d(in_ch, 1, kernel_size=1, stride=1)\n        \n    def forward(self, x):\n        input_x = x\n        \n        x = self.conv(x)\n        x = torch.sigmoid(x)\n        \n        x = torch.mul(input_x, x)\n        \n        return x\n\nclass SCSE(nn.Module):\n    def __init__(self, in_ch, r):\n        super(SCSE, self).__init__()\n        \n        self.cSE = CSE(in_ch, r)\n        self.sSE = SSE(in_ch)\n        \n    def forward(self, x):\n        cSE = self.cSE(x)\n        sSE = self.sSE(x)\n        \n        x = torch.add(cSE, sSE)\n        \n        return x\n\nclass WAPConv(nn.Module):\n    def __init__(self, in_ch: int, out_ch: int, dropout: bool = False):\n        super(WAPConv, self).__init__()\n        \n        self._dropout = dropout\n        \n        self.conv = nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n        self.bn = nn.BatchNorm2d(out_ch)\n        \n    def forward(self, x: torch.Tensor):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = F.relu(x)\n\n        if self._dropout:\n            x = F.dropout(x, 0.2)\n        \n        return x\n\nclass WAPBlock(nn.Module):\n    def __init__(self, in_ch: int, out_ch: int, dropout: bool = False):\n        super(WAPBlock, self).__init__()\n        \n        self.conv_1 = WAPConv(in_ch, out_ch, dropout=dropout)\n        self.conv_2 = WAPConv(out_ch, out_ch, dropout=dropout)\n        self.conv_3 = WAPConv(out_ch, out_ch, dropout=dropout)\n        self.conv_4 = WAPConv(out_ch, out_ch, dropout=dropout)\n\n        self.pool = nn.MaxPool2d(2, 2)\n\n    def forward(self, x: torch.Tensor):\n        x = self.conv_1(x)\n        x = self.conv_2(x)\n        x = self.conv_3(x)\n        x = self.conv_4(x)\n        \n        x = self.pool(x)\n\n        return x\n\n# Can't be pretrained; param is only for compatibility\ndef WAPBackbone(pretrained: bool = False):\n    model = nn.Sequential(\n        WAPBlock(3, 32),\n        WAPBlock(32, 64),\n        WAPBlock(64, 64),\n        WAPBlock(64, 128, dropout=True)\n    )\n    \n    return model\n\n# From https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py\n# Can't be pretrained; param is only for compatibility\ndef densenet(pretrained: bool = False):\n    model =  torchvision.models.DenseNet(growth_rate=24, block_config=(32, 32, 32), num_init_features=48)\n    \n    return model\n\nclass Encoder(nn.Module, Registrable):\n    def __init__(self, pretrained: bool = False) -> None:\n        super().__init__()\n        \n        self._pretrained = pretrained\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        raise NotImplementedError()\n\n    def get_output_dim(self) -> int:\n        raise NotImplementedError()\n        \n    def get_feature_map_size(self) -> int:\n        raise NotImplementedError()\n        \n@Encoder.register('backbone')\nclass BackboneEncoder(Encoder):\n    def __init__(self, encoder_type: str = 'renset18', encoder_height: int = 4, encoder_width: int = 16, pretrained: bool = False, custom_in_conv: bool = False) -> None:\n        super().__init__(pretrained=pretrained)\n        \n        self._encoder_type = encoder_type\n        \n        self._encoder_height = encoder_height\n        self._encoder_width = encoder_width\n        \n        self._custom_in_conv = custom_in_conv\n        \n        self._backbones = {\n            'vgg16': {\n                'model': torchvision.models.vgg16,\n                'encoder_dim': 512\n            },\n            'resnet18': { # 4 x 16\n                'model': torchvision.models.resnet18,\n                'encoder_dim': 512\n            },\n            'resnet50': { # 4 x 16\n                'model': torchvision.models.resnet50,\n                'encoder_dim': 2048\n            },\n            'densenet': { # 8 x 32\n                'model': densenet,\n                'encoder_dim': 1356\n            },\n            'WAP': { # 8 x 32\n                'model': WAPBackbone,\n                'encoder_dim': 128\n            },\n            'Im2latex': { # 14 x 62\n                'model': Im2latexBackbone,\n                'encoder_dim': 512\n            },\n            'smallResnet18': { # 8 x 32\n                'model': torchvision.models.resnet18,\n                'encoder_dim': 256\n            },\n        }\n        \n        self._backbone = self._backbones[self._encoder_type]['model'](pretrained=self._pretrained)\n        self._encoder_dim = self._backbones[self._encoder_type]['encoder_dim']\n        \n        if self._custom_in_conv:\n            self._backbone._modules['conv1'] = nn.Conv2d(3, 64, 3, padding=1)\n        \n        modules = list(self._backbone.children())\n        \n        if self._encoder_type == 'densenet':\n            modules = modules[0][:-1]\n        elif self._encoder_type == 'vgg16':\n            modules = modules[:-1]\n        elif self._encoder_type == 'smallResnet18':\n            modules = modules[:-3]\n        elif self._encoder_type == 'resnet18' or self._encoder_type == 'resnet50':\n            modules = modules[:-2]\n\n            # Add SCSE between resnet blocks\n            modules = nn.Sequential(\n                *modules[:5],\n                SCSE(64, 16),\n                *modules[5],\n                SCSE(128, 16),\n                *modules[6],\n                SCSE(256, 16),\n                *modules[7],\n                SCSE(512, 16)\n            )\n\n        self._encoder = nn.Sequential(\n            *modules,\n            nn.AdaptiveAvgPool2d((self._encoder_height, self._encoder_width))\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Encode image\n        x = self._encoder(x)\n\n        # Flatten image\n        # Shape: (batch_size, height * width, encoder_dim)\n        x = x.view(x.shape[0], -1, x.shape[1])\n\n        return x\n\n    def get_output_dim(self) -> int:\n        return self._encoder_dim\n    \n    def get_feature_map_size(self) -> int:\n        return self._encoder_height * self._encoder_width\n        \n@Encoder.register('lstm')\nclass LstmEncoder(Encoder):\n    # Don't set hidden_size manually\n    def __init__(self, encoder: Encoder, hidden_size: int = 512, layers: int = 1, bidirectional: bool = False) -> None:\n        super().__init__(pretrained=False)\n\n        self._encoder = encoder\n        \n        self._hidden_size = hidden_size\n        self._layers = layers\n        self._bidirectional = bidirectional\n        \n        self._lstm = nn.LSTM(input_size=self._encoder.get_output_dim(), hidden_size=self._hidden_size, num_layers=self._layers, batch_first=True, \n                             bidirectional=self._bidirectional)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Encode image\n        # Shape: (batch_size, height * width, encoder_dim)\n        x = self._encoder(x)\n        \n        # Encode encoded feature map with (bi)lstm\n        # Shape: (batch_size, height * width, num_directions * hidden_size)\n        x, _ = self._lstm(x)\n\n        if self._bidirectional:\n            # Shape: (batch_size, height * width, num_directions, hidden_size)\n            x = x.view(-1, x.shape[1], 2, self._hidden_size)\n\n            # Add directions and reverse bidirectional part\n#             x = x[:, :, 0, :] + torch.from_numpy(np.flip(x[:, :, 1, :].detach().cpu().numpy(), axis=-1).copy()).to(device)\n            x = x[:, :, 0, :] + x[:, :, 1, :]\n\n        return x\n    \n    @classmethod\n    def from_params(cls, vocab: Vocabulary, params: Params) -> 'BasicTextFieldEmbedder':  # type: ignore\n        encoder = params.pop(\"encoder\")\n        layers = params.pop(\"layers\")\n        bidirectional = params.pop(\"bidirectional\")\n        \n        encoder = Encoder.from_params(vocab=vocab, params=encoder)\n        \n        return cls(encoder, encoder._encoder_dim, layers, bidirectional)\n\n    @overrides\n    def get_output_dim(self) -> int:\n        return self._hidden_size\n    \n    def get_feature_map_size(self) -> int:\n        return self._encoder._encoder_height * self._encoder._encoder_width\n    \nclass Im2latexBlock(nn.Module):\n    def __init__(self, in_ch: int, out_ch: int, padding: int, bn: bool = True, pool: bool = False, pool_stride: Tuple[int, int] = None):\n        super(Im2latexBlock, self).__init__()\n        \n        self._bn = bn\n        self._pool = pool\n\n        self.conv = nn.Conv2d(in_ch, out_ch, 3, 1, 1 if padding else 0)\n        \n        if self._bn:\n            self.bn = nn.BatchNorm2d(out_ch)\n            \n        if self._pool:\n            self.pool = nn.MaxPool2d(pool_stride, pool_stride)\n\n    def forward(self, x: torch.Tensor):\n        x = self.conv(x)\n\n        if self._bn:\n            x = self.bn(x)\n\n        x = F.relu(x)\n\n        if self._pool:\n            x = self.pool(x)\n\n        return x\n\n# Can't be pretrained; param is only for compatibility\ndef Im2latexBackbone(pretrained: bool = False):\n    model = nn.Sequential(\n        Im2latexBlock(3, 64, 1, False, True, (2, 2)),\n        Im2latexBlock(64, 128, 1, False, True, (2, 2)),\n        Im2latexBlock(128, 256, 1, True, False),\n        Im2latexBlock(256, 256, 1, False, True, (1, 2)),\n        Im2latexBlock(256, 512, 1, True, True, (2, 1)),\n        Im2latexBlock(512, 512, 0, True, False)\n    )\n    \n    return model\n\n@Encoder.register('Im2latex')\nclass Im2latexEncoder(Encoder):\n    # Don't set hidden_size manually\n    def __init__(self, encoder: Encoder, hidden_size: int = 512, layers: int = 1, bidirectional: bool = False) -> None:\n        super().__init__(pretrained=False)\n       \n        self._hidden_size = hidden_size\n        self._layers = layers\n        self._bidirectional = bidirectional\n        \n        self._num_directions = 2 if self._bidirectional else 1\n        \n        self._encoder = encoder\n        \n        self._row_encoder = nn.GRU(input_size=self._encoder.get_output_dim(), hidden_size=self._hidden_size, num_layers=self._layers, batch_first=True, \n                                    bidirectional=self._bidirectional)\n        \n        self._positional_embeddings = nn.Embedding(self._encoder._encoder_height, self._hidden_size)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Encode image\n        # Shape: (batch_size, height * width, encoder_dim)\n        x = self._encoder(x)\n        \n        # Shape: (batch_size, encoder_dim, height, width)\n        x = x.view(-1, self._encoder._encoder_dim, self._encoder._encoder_height, self._encoder._encoder_width)\n\n        # Shape: (batch_size, hidden_size, height, width)\n        encoded_rows = torch.zeros((x.shape[0], self._hidden_size, self._encoder._encoder_height, self._encoder._encoder_width), device=device)\n        \n        # Go over each row\n        for i in range(x.shape[2]):\n            # Get row\n            # Shape: (batch_size, width, encoder_dim)\n            row = x[:, :, i].transpose(1, 2)\n            \n            # Get positional embeddings for row\n            # Shape: (1, hidden_size)\n            positional_embedding = self._positional_embeddings(torch.LongTensor([i]).to(device))\n\n            # Duplicate positional embeddings for each element in batch\n            # Shape: (layers * num_directions, batch_size, hidden_size)\n            positional_embedding = positional_embedding.view(1, 1, self._hidden_size).repeat(self._layers * self._num_directions, x.shape[0], 1)\n            \n            # Encode row\n            # Shape: (batch_size, width, num_directions * hidden_size)\n            encoded_row, _ = self._row_encoder(row, positional_embedding)\n            \n            if self._bidirectional:\n                # Shape: (batch_size, width, 2, hidden_size)\n                encoded_row = encoded_row.view(-1, encoded_row.shape[1], 2, self._hidden_size)\n\n                # Add bidirectional directions\n                # Shape: (batch_size, width, hidden_size)\n                encoded_row = encoded_row[:, :, 0, :] + encoded_row[:, :, 1, :]\n                # Reverse bidirectional direction\n#                 encoded_row = encoded_row[:, :, 0, :] + torch.from_numpy(np.flip(encoded_row[:, :, 1, :].detach().cpu().numpy(), axis=-1).copy()).to(device)\n\n            # Shape: (batch_size, hidden_size, width)\n            encoded_rows[:, :, i, :] = encoded_row.transpose(1, 2)\n\n        # Shape: (batch_size, height * with, hidden_size)\n        x = encoded_rows.view(-1, self._encoder.get_feature_map_size(), self._hidden_size)\n    \n        return x\n\n    @classmethod\n    def from_params(cls, vocab: Vocabulary, params: Params) -> 'BasicTextFieldEmbedder':  # type: ignore\n        encoder = params.pop(\"encoder\")\n        layers = params.pop(\"layers\")\n        bidirectional = params.pop(\"bidirectional\")\n        \n        encoder = Encoder.from_params(vocab=vocab, params=encoder)\n        \n        return cls(encoder, encoder._encoder_dim, layers, bidirectional)\n\n    def get_output_dim(self) -> int:\n        return self._hidden_size\n    \n    def get_feature_map_size(self) -> int:\n        return self._encoder._encoder_height * self._encoder._encoder_width\n\n# from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(int(in_planes), int(out_planes), kernel_size=3, stride=stride,padding=1, bias=False)\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(int(in_planes), int(out_planes), kernel_size=1, stride=stride, bias=False)\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n    \n# From https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py\nclass _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n        self.add_module('relu1', nn.ReLU(inplace=True)),\n        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n                        growth_rate, kernel_size=1, stride=1, bias=False)),\n        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module('relu2', nn.ReLU(inplace=True)),\n        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                        kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\nclass _DenseBlock(nn.Sequential):\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n            self.add_module('denselayer%d' % (i + 1), layer)\n\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features, pool=True):\n        super(_Transition, self).__init__()\n        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n        self.add_module('relu', nn.ReLU(inplace=True))\n        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        if pool == True:\n            self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n\n# pretrained is only for compatibility\nclass MultiscaleDenseNet(nn.Module):\n    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n                 num_init_features=64, bn_size=4, drop_rate=0.2, num_classes=1000, pretrained=False):\n\n        super(MultiscaleDenseNet, self).__init__()\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n            ('norm0', nn.BatchNorm2d(num_init_features)),\n            ('relu0', nn.ReLU(inplace=True)),\n            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n        ]))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            pool = True\n            if i == len(block_config) - 1:\n                pool = False\n            \n            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n            self.features.add_module('denseblock%d' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n\n            trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2, pool=pool)\n                \n            self.features.add_module('transition%d' % (i + 1), trans)\n            num_features = num_features // 2\n\n            # Add SCSE\n            scse = nn.Sequential(SCSE(num_features, 16))\n            self.features.add_module('scse%d' % (i + 1), scse)\n            \n        # Multiscale branch\n\n        self.main_branch = nn.Sequential(\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            _DenseBlock(num_layers=32, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate),\n            SCSE(1356, 16)\n        )\n\n        self.multiscale_branch = nn.Sequential(\n            _DenseBlock(num_layers=16, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate),\n            SCSE(972, 16)\n        )\n\n        # Official init from torch repo.\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        features = self.features(x)\n        main_features = self.main_branch(features)\n        multiscale_features = self.multiscale_branch(features)\n\n        return [main_features, multiscale_features]\n\n@Encoder.register('multiscale')\nclass MultiscaleEncoder(Encoder):\n    def __init__(self, encoder_type: str = 'renset18', encoder_height: int = 4, encoder_width: int = 16, pretrained: bool = False) -> None:\n        super().__init__(pretrained=pretrained)\n        \n        self._encoder_type = encoder_type\n        \n        self._encoder_height = encoder_height\n        self._encoder_width = encoder_width\n        \n        self._backbones = {\n            'resnet18': {\n                'model': torchvision.models.resnet18,\n                'encoder_dim': [512]\n            },\n            'resnet50': {\n                'model': torchvision.models.resnet50,\n                'encoder_dim': [2048]\n            },\n            'densenet': {\n                'model': MultiscaleDenseNet,\n                'encoder_dim': [1356, 972]\n            }\n        }\n                \n        if self._encoder_type == 'resnet18' or self._encoder_type == 'resnet50':\n            self._backbone = self._backbones[self._encoder_type]['model'](pretrained=self._pretrained)\n            self._encoder_dim = self._backbones[self._encoder_type]['encoder_dim'][0]\n\n            # Common conv blocks\n            self._encoder = nn.Sequential(\n                *list(self._backbone.children())[:-3]\n            )\n\n            # Last conv block\n            self._main_branch = nn.Sequential(*list(self._backbone.children())[-3])\n\n            # Uses 1x1 convs to convert identity to correct num of channels\n            self._identity_conv = nn.Sequential(\n                conv1x1(self._encoder_dim / 2, self._encoder_dim),\n                nn.BatchNorm2d(self._encoder_dim),\n            )\n\n            # Last conv block without pool and not pretrained\n            self._multiscale_branch = nn.Sequential(\n                BasicBlock(self._encoder_dim / 2, self._encoder_dim, downsample=self._identity_conv),\n                BasicBlock(self._encoder_dim, self._encoder_dim)\n            )\n        else:\n            self._backbone = self._backbones[self._encoder_type]['model'](growth_rate=24, block_config=(32, 32), num_init_features=48)\n            self._encoder_dim = self._backbones[self._encoder_type]['encoder_dim'][0]\n\n            self._encoder = self._backbone\n\n    def forward(self, x: torch.Tensor):\n        # Encode image through common conv blocks\n        # Shape: (batch_size, channels, height * 2, width * 2)\n        x = self._encoder(x)\n\n        if self._encoder_type == 'resnet18' or self._encoder_type == 'resnet50':\n            # Shape: (batch_size, channels, height, width)\n            main_features = self._main_branch(x)\n\n            # Get multiscale features\n            # Shape: (batch_size, channels, height * 2, width * 2)\n            multiscale_features = self._multiscale_branch(x)            \n        else:\n            main_features, multiscale_features = x[0], x[1]\n            \n        # Flatten features\n        # Shape: (batch_size, height * width, encoder_dim)\n        main_features = main_features.view(main_features.shape[0], -1, main_features.shape[1])\n        # Shape: (batch_size, height * 2 * width * 2, encoder_dim)\n        multiscale_features = multiscale_features.view(multiscale_features.shape[0], -1, multiscale_features.shape[1])\n\n        return [main_features, multiscale_features]\n    \n    def get_output_dim(self) -> int:\n        return self._encoder_dim\n    \n    def get_feature_map_size(self) -> int:\n        return self._encoder_height * self._encoder_width\n\n#DEPRECATED\n@Encoder.register('multiscale-lstm')\nclass LstmEncoder(Encoder):\n    # Don't set hidden_size manually\n    def __init__(self, encoder: Encoder, hidden_size: int = 256, layers: int = 1, bidirectional: bool = False) -> None:\n        super().__init__(pretrained=False)\n\n        self._encoder = encoder\n        \n        self._hidden_size = hidden_size\n        self._layers = layers\n        self._bidirectional = bidirectional\n        \n        self._lstm = nn.LSTM(input_size=1356, hidden_size=1356, num_layers=self._layers, batch_first=True, \n                             bidirectional=self._bidirectional)\n        \n        self._lstm2 = nn.LSTM(input_size=972, hidden_size=972, num_layers=self._layers, batch_first=True, \n                             bidirectional=self._bidirectional)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Encode image\n        # Shape: (batch_size, height * width, encoder_dim)\n        x = self._encoder(x)\n        \n        # Encode encoded main and dense feature map with (bi)lstm\n        # Shape: (batch_size, height * width, num_directions * hidden_size)\n        x_1, _ = self._lstm(x[0])\n        x_2, _ = self._lstm2(x[1])\n\n        if self._bidirectional:\n            # Shape: (batch_size, height * width, num_directions, hidden_size)\n            x_1 = x_1.view(-1, x_1.shape[1], 2, self._hidden_size)\n            x_2 = x_2.view(-1, x_2.shape[1], 2, self._hidden_size)\n\n            # Add directions and reverse bidirectional part\n#             x_1 = x_1[:, :, 0, :] + torch.from_numpy(np.flip(x_1[:, :, 1, :].detach().cpu().numpy(), axis=-1).copy()).to(device)\n#             x_2 = x_2[:, :, 0, :] + torch.from_numpy(np.flip(x_2[:, :, 1, :].detach().cpu().numpy(), axis=-1).copy()).to(device)\n\n            x_1 = x_1[:, :, 0, :] + x_1[:, :, 1, :]\n            x_2 = x_2[:, :, 0, :] + x_2[:, :, 1, :]\n\n        return [x_1, x_2]\n    \n    @classmethod\n    def from_params(cls, vocab: Vocabulary, params: Params) -> 'BasicTextFieldEmbedder':  # type: ignore\n        encoder = params.pop(\"encoder\")\n        layers = params.pop(\"layers\")\n        bidirectional = params.pop(\"bidirectional\")\n        \n        encoder = Encoder.from_params(vocab=vocab, params=encoder)\n        \n        return cls(encoder, encoder._encoder_dim, layers, bidirectional)\n    \n    def get_output_dim(self) -> int:\n        return self._encoder._encoder_dim\n    \n    def get_feature_map_size(self) -> int:\n        return self._encoder._encoder_height * self._encoder._encoder_width","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d438cc68875a0ccbb7eee7f710c1d592542971f6"},"cell_type":"code","source":"%%writefile math_handwriting_recognition/attention.py\nimport os\nimport random\nfrom typing import Dict, Tuple\nfrom overrides import overrides\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchvision\n\nimport allennlp\n\nfrom allennlp.common import Registrable, Params\nfrom allennlp.data.vocabulary import Vocabulary\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nclass CaptioningAttention(nn.Module, Registrable):\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, x: torch.Tensor, h: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        raise NotImplementedError()\n\n    def get_output_dim(self) -> int:\n        raise NotImplementedError()\n\n@CaptioningAttention.register('image-captioning')\nclass ImageCaptioningAttention(CaptioningAttention):\n    def __init__(self, encoder_dim: int = 512, decoder_dim: int = 256, attention_dim: int = 256, doubly_stochastic_attention: bool = True) -> None:\n        super().__init__()\n                \n        self._encoder_dim = encoder_dim\n        self._decoder_dim = decoder_dim\n        self._attention_dim = attention_dim\n        \n        self._doubly_stochastic_attention = doubly_stochastic_attention\n        \n        self._encoder_attention = nn.Linear(self._encoder_dim, self._attention_dim)\n        self._decoder_attention = nn.Linear(self._decoder_dim, self._attention_dim)\n        self._attention = nn.Linear(self._attention_dim, 1)\n\n        if self._doubly_stochastic_attention:\n            self._f_beta = nn.Linear(self._decoder_dim, self._encoder_dim)\n\n    @overrides\n    def forward(self, x: torch.Tensor, h: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        # Shape: (batch_size, height * width, attention_dim)\n        encoder_attention = self._encoder_attention(x)\n        # Shape: (batch_size, 1, attention_dim)\n        decoder_attention = self._decoder_attention(h).unsqueeze(1)\n\n        # Shape: (batch_size, height * width)\n        # Can't concat attention since encoder returns h*w and decoder returns 1\n        attention = self._attention(torch.tanh(encoder_attention + decoder_attention)).squeeze(2)\n\n        # No need for masked softmax since all encoder pixels are available and hidden state of rnn isn't masked\n        # Shape: (batch_size, h * w, 1)\n        attention_weights = torch.softmax(attention, dim=1).unsqueeze(2)\n\n        # Shape: (batch_size, encoder_dim)\n        attention = (x * attention_weights).sum(dim=1)\n        \n        if self._doubly_stochastic_attention:     \n            # Shape: (batch_size, encoder_dim)\n            gate = torch.sigmoid(self._f_beta(h))\n            # Shape: (batch_size, encoder_dim)\n            attention = gate * attention\n        \n        return attention, attention_weights\n    \n    @overrides\n    def get_output_dim(self) -> int:\n        return self._encoder_dim\n\n@CaptioningAttention.register('WAP')\nclass WAPAttention(CaptioningAttention):\n    def __init__(self, encoder_dim: int = 512, decoder_dim: int = 256, attention_dim: int = 256, kernel_size: int = 5, padding: int=2) -> None:\n        super().__init__()\n                \n        self._encoder_dim = encoder_dim\n        self._decoder_dim = decoder_dim\n        self._attention_dim = attention_dim\n        self._kernel_size = kernel_size\n        \n        self._encoder_attention = nn.Linear(self._encoder_dim, self._attention_dim)\n        self._decoder_attention = nn.Linear(self._decoder_dim, self._attention_dim)\n        \n        # If kernel size is changed, padding needs to also change\n        # Not sure if original uses padding; needed here since need same dimension inputs to attention\n        self._coverage = nn.Conv2d(1, self._attention_dim, kernel_size, padding=padding)\n        self._coverage_attention = nn.Linear(self._attention_dim, self._attention_dim)\n        \n        self._attention = nn.Linear(self._attention_dim, 1)\n\n    @overrides\n    def forward(self, x: torch.Tensor, h: torch.Tensor, sum_attention_weights: torch.Tensor, height: int = 8) -> Tuple[torch.Tensor, torch.Tensor]:\n        # Shape: (batch_size, height * width, attention_dim)\n        encoder_attention = self._encoder_attention(x)\n\n        # Shape: (batch_size, 1, attention_dim)\n        decoder_attention = self._decoder_attention(h).unsqueeze(1)\n\n        # Get coverage over sum correctly when batch size at timestep isn't local batch size \n        # Need to clone sum_attention_weights since it's modified by an in-place operation \n        # Assumes 4:1 aspect ratio\n        # Shape: (batch_size, height * width, attention_dim)\n        # Shape: (batch_size, height * width, attention_dim)\n        coverage = self._coverage(sum_attention_weights[:encoder_attention.shape[0]].view(-1,1, height, height * 4).clone()).view(-1, height * height * 4, self._attention_dim)\n        coverage_attention = self._coverage_attention(coverage)\n\n        # Shape: (batch_size, height * width)\n        attention = self._attention(torch.tanh(encoder_attention + decoder_attention + coverage_attention)).squeeze(2)\n\n        # No need for masked softmax since all encoder pixels are available and hidden state of rnn isn't masked\n        # Shape: (batch_size, h * w, 1)\n        attention_weights = torch.softmax(attention, dim=1).unsqueeze(2)\n\n        # Update sum correctly when batch size at timestep isn't local batch size \n        # Shape: (batch_size, h * w)\n        sum_attention_weights[:attention_weights.shape[0]] += attention_weights.view(-1, attention_weights.shape[1])\n\n        # Shape: (batch_size, encoder_dim)\n        attention = (x * attention_weights).sum(dim=1)\n        \n        return attention, attention_weights, sum_attention_weights\n    \n    @overrides\n    def get_output_dim(self) -> int:\n        return self._encoder_dim\n\n@CaptioningAttention.register('multiscale')\nclass MultiscaleAttention(CaptioningAttention):\n    def __init__(self, main_attention: CaptioningAttention, multiscale_attention: CaptioningAttention, height_1: int = 4, height_2: int = 8) -> None:\n        super().__init__()\n\n        self._main_attention = main_attention\n        self._multiscale_attention = multiscale_attention\n        \n        self._height_1 = height_1\n        self._height_2 = height_2\n\n    @overrides\n    def forward(self, x: torch.Tensor, h: torch.Tensor, sum_attention_weights_0: torch.Tensor, sum_attention_weights_1: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        main_features, multiscale_features = x[0], x[1]\n        \n        main_attention, main_attention_weights, sum_attention_weights_0 = self._main_attention(main_features, h, sum_attention_weights_0, height=self._height_1)\n        multiscale_attention, multiscale_attention_weights, sum_attention_weights_1 = self._multiscale_attention(multiscale_features, h, sum_attention_weights_1, height=self._height_2)\n        \n        attention = torch.cat([main_attention, multiscale_attention], dim=1)\n        \n        return attention, (main_attention_weights, multiscale_attention_weights), sum_attention_weights_0, sum_attention_weights_1\n    \n    @overrides\n    def get_output_dim(self) -> int:\n        return self._main_attention._encoder_dim + self._multiscale_attention._encoder_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"391d5c22017f57975e2de1f34b12100eab86519a"},"cell_type":"code","source":"%%writefile math_handwriting_recognition/decoder.py\nimport os\nimport random\nfrom typing import Dict, Tuple\nfrom overrides import overrides\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchvision\n\nimport allennlp\n\nfrom allennlp.common import Registrable, Params\n\nfrom allennlp.data.vocabulary import Vocabulary\n\nfrom allennlp.modules.token_embedders import Embedding\n\nfrom math_handwriting_recognition.attention import CaptioningAttention\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nclass CaptioningDecoder(nn.Module, Registrable):\n    def __init__(self, vocab: Vocabulary):\n        super(CaptioningDecoder, self).__init__()\n        \n        self.vocab = vocab\n        \n    def forward(self, x: torch.Tensor, h: torch.Tensor, c: torch.Tensor, predicted_indices: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        raise NotImplementedError()\n        \n    def get_output_dim(self) -> int:\n        raise NotImplementedError()\n\n    # Input dim is dim of h and c\n    def get_input_dim(self) -> int:\n        raise NotImplementedError()\n\n@CaptioningDecoder.register('image-captioning')\nclass ImageCaptioningDecoder(CaptioningDecoder):\n    def __init__(self, vocab: Vocabulary, attention: CaptioningAttention, embedding_dim:int = 256, decoder_dim:int = 256):\n        super(ImageCaptioningDecoder, self).__init__(vocab=vocab)\n        \n        self._vocab_size = self.vocab.get_vocab_size()\n        self._embedding_dim = embedding_dim\n        self._decoder_dim = decoder_dim\n\n        self._embedding = Embedding(self._vocab_size, self._embedding_dim)\n        self._attention = attention\n        self._decoder_cell = nn.LSTMCell(self._embedding.get_output_dim() + self._attention.get_output_dim(), self._decoder_dim)\n        self._linear = nn.Linear(self._decoder_dim, self._vocab_size)\n\n    @overrides\n    def forward(self, x: torch.Tensor, h: torch.Tensor, c: torch.Tensor, predicted_indices: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        # Shape: (batch_size, embedding_dim)\n        embedding = self._embedding(predicted_indices).float().view(-1, self._embedding_dim)\n        \n        # Shape: (batch_size, encoder_dim) (batch_size, h * w, 1)\n        attention, attention_weights = self._attention(x, h)\n\n        ## Change to not use teacher forcing all the time\n        # Shape: (batch_size, decoder_dim) (batch_size, decoder_dim)\n        h, c = self._decoder_cell(torch.cat([attention, embedding], dim=1), (h, c))\n        \n        # Get output predictions (one per character in vocab)\n        # Shape: (batch_size, vocab_size)\n        preds = self._linear(h)\n\n        return h, c, preds, attention_weights\n    \n    @overrides\n    def get_output_dim(self) -> int:\n        return self._vocab_size\n    \n    @overrides\n    def get_input_dim(self) -> int:\n        return self._decoder_dim\n\n@CaptioningDecoder.register('WAP')\nclass WAPDecoder(CaptioningDecoder):\n    def __init__(self, vocab: Vocabulary, attention: CaptioningAttention, embedding_dim:int = 256, decoder_dim:int = 256):\n        super(WAPDecoder, self).__init__(vocab=vocab)\n        \n        self._vocab_size = self.vocab.get_vocab_size()\n        self._embedding_dim = embedding_dim\n        self._decoder_dim = decoder_dim\n\n        self._embedding = Embedding(self._vocab_size, self._embedding_dim)\n        self._attention = attention\n        self._decoder_cell = nn.GRUCell(self._embedding.get_output_dim() + self._attention.get_output_dim(), self._decoder_dim)\n        self._linear = nn.Linear(self._decoder_dim, self._vocab_size)\n\n    @overrides\n    def forward(self, x: torch.Tensor, h: torch.Tensor, predicted_indices: torch.Tensor, sum_attention_weights: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        # Shape: (batch_size, embedding_dim)\n        embedding = self._embedding(predicted_indices).float().view(-1, self._embedding_dim)\n        \n        # Shape: (batch_size, encoder_dim) (batch_size, h * w, 1) (batch_size, h * w)\n        attention, attention_weights, sum_attention_weights = self._attention(x, h, sum_attention_weights)\n\n        ## Change to not use teacher forcing all the time\n        # Shape: (batch_size, decoder_dim) (batch_size, decoder_dim)\n        h = self._decoder_cell(torch.cat([attention, embedding], dim=1), h)\n        \n        # Get output predictions (one per character in vocab)\n        # Shape: (batch_size, vocab_size)\n        preds = self._linear(h)\n\n        return h, preds, attention_weights, sum_attention_weights\n    \n    @overrides\n    def get_output_dim(self) -> int:\n        return self._vocab_size\n    \n    @overrides\n    def get_input_dim(self) -> int:\n        return self._decoder_dim\n\n@CaptioningDecoder.register('multiscale')\nclass MultiscaleDecoder(CaptioningDecoder):\n    def __init__(self, vocab: Vocabulary, attention: CaptioningAttention, embedding_dim: int = 256, decoder_dim:int = 256):\n        super(MultiscaleDecoder, self).__init__(vocab=vocab)\n\n        self._vocab_size = self.vocab.get_vocab_size()\n        self._embedding_dim = embedding_dim\n        self._decoder_dim = decoder_dim\n                \n        self._embedding = Embedding(self._vocab_size, self._embedding_dim)\n        self._dropout = nn.Dropout(0.1)\n        # Output size of state cell must be decoder dim since state is transformed by the state cell\n        self._state_cell = nn.GRUCell(self._embedding.get_output_dim(), self._decoder_dim)\n\n        self._attention = attention\n        self._decoder_cell = nn.GRUCell(self._attention.get_output_dim(), self._decoder_dim)\n\n        self._linear = nn.Linear(self._decoder_dim, self._vocab_size)\n\n    @overrides\n    def forward(self, x: torch.Tensor, h: torch.Tensor, predicted_indices: torch.Tensor, sum_attention_weights_0: torch.Tensor, sum_attention_weights_1: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        # Shape: (batch_size, embedding_dim)\n        embedding = self._embedding(predicted_indices).float().view(-1, self._embedding_dim)\n        embedding = self._dropout(embedding)\n        \n        # Shape: (batch_size, decoder_dim)\n        h = self._state_cell(embedding, h)\n\n        # Shape: (batch_size, encoder_dim) (batch_size, h * w, 1)\n        attention, attention_weights, sum_attention_weights_0, sum_attention_weights_1 = self._attention(x, h, sum_attention_weights_0, sum_attention_weights_1)\n\n        ## Change to not use teacher forcing all the time\n        # Shape: (batch_size, decoder_dim) (batch_size, decoder_dim)\n        h = self._decoder_cell(attention, h)\n\n        # Get output predictions (one per character in vocab)\n        # Shape: (batch_size, vocab_size)\n        preds = self._linear(h)\n\n        return h, preds, attention_weights, sum_attention_weights_0, sum_attention_weights_1\n    \n    @overrides\n    def get_output_dim(self) -> int:\n        return self._vocab_size\n    \n    @overrides\n    def get_input_dim(self) -> int:\n        return self._decoder_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53dcb854b4d1bcdf32869d792d19c2bada5a7797"},"cell_type":"code","source":" %%writefile math_handwriting_recognition/model.py\nimport os\nimport random\nfrom typing import Dict, Tuple\nfrom overrides import overrides\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchvision\n\nimport allennlp\n\nfrom allennlp.common import Registrable, Params\nfrom allennlp.common.util import START_SYMBOL, END_SYMBOL\n\nfrom allennlp.data.vocabulary import Vocabulary\n\nfrom allennlp.models import Model\n\nfrom allennlp.modules.token_embedders import Embedding\n\nfrom allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\nfrom allennlp.nn.beam_search import BeamSearch\n\nfrom allennlp.training.metrics import F1Measure, BLEU\n\nfrom math_handwriting_recognition.metrics import Exprate\nfrom math_handwriting_recognition.encoder import Encoder\nfrom math_handwriting_recognition.decoder import CaptioningDecoder\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n@Model.register('image-captioning')\nclass ImageCaptioning(Model):\n    def __init__(self, vocab: Vocabulary, encoder: Encoder, decoder: CaptioningDecoder, max_timesteps: int = 75, teacher_forcing: bool = True, scheduled_sampling_ratio: float = 1, beam_size: int = 10) -> None:\n        super().__init__(vocab)\n\n        self._start_index = self.vocab.get_token_index(START_SYMBOL)\n        self._end_index = self.vocab.get_token_index(END_SYMBOL)\n        self._pad_index = self.vocab.get_token_index('@@PADDING@@')\n\n        self._max_timesteps = max_timesteps\n        self._teacher_forcing = teacher_forcing\n        self._scheduled_sampling_ratio = scheduled_sampling_ratio\n        self._beam_size = beam_size\n\n        self._encoder = encoder\n        self._decoder = decoder\n\n        self._init_h = nn.Linear(self._encoder.get_output_dim(), self._decoder.get_input_dim())\n        self._init_c = nn.Linear(self._encoder.get_output_dim(), self._decoder.get_input_dim())\n\n        self.beam_search = BeamSearch(self._end_index, self._max_timesteps, self._beam_size)\n\n        self._bleu = BLEU(exclude_indices={self._start_index, self._end_index, self._pad_index})\n        self._exprate = Exprate(self._end_index, self.vocab)\n\n        self._attention_weights = None\n        \n    def _init_hidden(self, encoder: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        mean_encoder = encoder.mean(dim=1)\n        \n        # Shape: (batch_size, decoder_dim)\n        initial_h = self._init_h(mean_encoder)\n        # Shape: (batch_size, decoder_dim)\n        initial_c = self._init_c(mean_encoder)\n\n        return initial_h, initial_c\n    \n    def _decode(self, state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        # Get data from state\n        metadata = state['metadata']\n        x = state['x']\n        h = state['h']\n        c = state['c']\n        label = state['label']\n        mask = state['mask']\n        \n        # Get actual size of current batch\n        local_batch_size = x.shape[0]\n\n        # Sort data to be able to only compute relevent parts of the batch at each timestep\n        # Shape: (batch_size)\n        lengths = mask.sum(dim=1)\n        # Shape: (batch_size) (batch_size)\n        sorted_lengths, indices = lengths.sort(dim=0, descending=True)\n        # Computing last timestep isn't necessary with labels since last timestep is eos token or pad token \n        timesteps = sorted_lengths[0] - 1\n\n        # Shape: (batch_size, ?)\n        # Shape: (batch_size, height * width, encoder_dim)\n        # Shape: (batch_size, decoder_dim)\n        # Shape: (batch_size, decoder_dim)\n        # Shape: (batch_size, timesteps)\n        # Shape: (batch_size, timesteps)\n        metadata = [metadata[i] for i in indices]\n        x = x[indices]\n        h = h[indices]\n        c = c[indices]\n        label = label[indices]        \n        mask = mask[indices]\n        \n        # Shape: (batch_size, 1)\n        predicted_indices = torch.LongTensor([[self._start_index]] * local_batch_size).to(device).view(-1, 1)\n        \n        # Shape: (batch_size, timesteps, vocab_size)\n        predictions = torch.zeros(local_batch_size, timesteps, self._decoder.get_output_dim(), device=device)\n        attention_weights = torch.zeros(local_batch_size, timesteps, self._encoder.get_feature_map_size(), device=device)\n        \n        for t in range(timesteps):\n            # Shape: (batch_offset)\n            batch_offset = sum([l > t for l in sorted_lengths.tolist()])\n\n            # Only compute data in valid timesteps\n            # Shape: (batch_offset, height * width, encoder_dim)\n            # Shape: (batch_offset, decoder_dim)\n            # Shape: (batch_offset, decoder_dim)\n            # Shape: (batch_offset, 1)\n            x_t = x[:batch_offset]\n            h_t = h[:batch_offset]\n            c_t = c[:batch_offset]\n            predicted_indices_t = predicted_indices[:batch_offset]\n            \n            # Decode timestep\n            # Shape: (batch_size, decoder_dim) (batch_size, decoder_dim) (batch_size, vocab_size), (batch_size, encoder_dim, 1)\n            h, c, preds, attention_weight = self._decoder(x_t, h_t, c_t, predicted_indices_t)\n            \n            # Get new predicted indices to pass into model at next timestep\n            # Use teacher forcing if chosen\n            if self._teacher_forcing:\n                # Send next timestep's label to next timestep\n                # Shape: (batch_size, 1)\n                predicted_indices = label[:batch_offset, t + 1].view(-1, 1)\n            else:\n                # Shape: (batch_size, 1)\n                predicted_indices = torch.argmax(preds, dim=1).view(-1, 1)\n            \n            # Save preds\n            predictions[:batch_offset, t, :] = preds\n            attention_weights[:batch_offset, t, :] = attention_weight.view(-1, self._encoder.get_feature_map_size())\n            \n        # Update state and add logits\n        state['metadata'] = metadata\n        state['x'] = x\n        state['h'] = h\n        state['c'] = c\n        state['label'] = label\n        state['mask'] = mask\n        state['attention_weights'] = attention_weights\n        state['logits'] = predictions\n            \n        return state\n    \n    def _beam_search_step(self, last_predictions: torch.Tensor, state: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        # Group_size is batch_size * beam_size except for first decoding timestep where it is batch_size\n        # Shape: (group_size, decoder_dim) (group_size, decoder_dim) (group_size, vocab_size)\n        h, c, predictions, attention_weights = self._decoder(state['x'], state['h'], state['c'], last_predictions)\n        \n        if self._attention_weights is not None:\n            attention_weights = attention_weights.view(-1, self._beam_size, 1, self._encoder.get_feature_map_size())\n            self._attention_weights = torch.cat([self._attention_weights, attention_weights[:, 0, :, :]], dim=1)\n        else:\n            attention_weights = attention_weights.view(-1, 1, self._encoder.get_feature_map_size())\n            self._attention_weights = attention_weights\n\n        # Update state\n        # Shape: (group_size, decoder_dim)\n        state['h'] = h\n        # Shape: (group_size, decoder_dim)\n        state['c'] = c\n\n        # Run log_softmax over logit predictions\n        # Shape: (group_size, vocab_size)\n        log_preds = F.log_softmax(predictions, dim=1)\n\n        return log_preds, state\n    \n    def _beam_search(self, state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        # Get data from state\n        x = state['x']\n        h = state['h']\n        c = state['c']\n        \n        # Get actual size of current batch\n        local_batch_size = x.shape[0]\n\n        # Beam search wants initial preds of shape: (batch_size)\n        # Shape: (batch_size)\n        initial_indices = torch.LongTensor([[self._start_index]] * local_batch_size).to(device).view(-1)\n        \n        state = {'x': x, 'h': h, 'c': c}\n        \n        # Timesteps returned aren't necessarily max_timesteps\n        # Shape: (batch_size, beam_size, timesteps), (batch_size, beam_size)\n        \n        self._attention_weights = None\n        \n        predictions, log_probabilities = self.beam_search.search(initial_indices, state, self._beam_search_step)\n\n        # Only keep best predictions from beam search\n        # Shape: (batch_size, timesteps)\n        predictions = predictions[:, 0, :].view(local_batch_size, -1)\n        \n        return predictions\n        \n    @overrides\n    def forward(self, metadata: object, img: torch.Tensor, label: Dict[str, torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n        # Encode the image\n        # Shape: (batch_size, height * width, encoder_dim)\n        x = self._encoder(img)\n\n        state = {'metadata': metadata, 'x': x}\n        # Compute loss on train and val\n        if label is not None:\n            # Initialize h and c\n            # Shape: (batch_size, decoder_dim)\n            state['h'], state['c'] = self._init_hidden(x)\n\n            # Convert label dict to tensor since label isn't an input to the model and get mask\n            # Shape: (batch_size, timesteps)\n            state['mask'] = get_text_field_mask(label).to(device)\n            # Shape: (batch_size, timesteps)\n            state['label'] = label['tokens']\n\n            # Decode encoded image and get loss on train and val\n            state = self._decode(state)\n\n            # Loss shouldn't be computed on start token\n            state['mask'] = state['mask'][:, 1:].contiguous()\n            state['target'] = state['label'][:, 1:].contiguous()\n\n            # Compute cross entropy loss\n            state['loss'] = sequence_cross_entropy_with_logits(state['logits'], state['target'], state['mask'])\n            # Doubly stochastic regularization\n            state['loss'] += ((1 - torch.sum(state['attention_weights'], dim=1)) ** 2).mean()\n\n        # Decode encoded image with beam search on val and test\n        if not self.training:\n            # (Re)initialize h and c\n            state['h'], state['c'] = self._init_hidden(state['x'])\n            \n            # Run beam search\n            state['out'] = self._beam_search(state)\n\n            # Save attention weights\n            state['attention_weights'] = self._attention_weights\n\n            # Compute validation scores\n            if 'label' in state:\n                self._bleu(state['out'], state['target'])\n                self._exprate(state['out'], state['target'])\n            \n        # Set out to logits while training\n        else:\n            state['out'] = state['logits']\n            \n        return state\n    \n    @overrides\n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        metrics: Dict[str, float] = {}\n\n        # Return Bleu score if possible\n        if not self.training:\n            metrics.update(self._bleu.get_metric(reset))\n            metrics.update(self._exprate.get_metric(reset))\n            \n        return metrics\n        \n    def _trim_predictions(self, predictions: torch.Tensor) -> torch.Tensor:\n        for b in range(predictions.shape[0]):\n            # Shape: (timesteps)\n            predicted_index = predictions[b]\n            # Set last predicted index to eos token in case there are no predicted eos tokens\n            predicted_index[-1] = self._end_index\n\n            # Get index of first eos token\n            # Shape: (timesteps)\n            mask = predicted_index == self._end_index\n            # Work around for pytorch not having an easy way to get the first non-zero index\n            eos_token_idx = list(mask.cpu().numpy()).index(1)\n            \n            # Set prediction at eos token's timestep to eos token\n            predictions[b, eos_token_idx] = self._end_index\n            # Replace all timesteps after first eos token with pad token\n            predictions[b, eos_token_idx + 1:] = self._pad_index\n\n        return predictions\n\n    @overrides\n    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        # Trim test preds to first eos token\n        # Shape: (batch_size, timesteps)\n        output_dict['out'] = self._trim_predictions(output_dict['out'])\n\n        return output_dict\n\n@Model.register('WAP')\nclass WAP(ImageCaptioning):\n    def __init__(self, vocab: Vocabulary, encoder: Encoder, decoder: CaptioningDecoder, max_timesteps: int = 75, teacher_forcing: bool = True, scheduled_sampling_ratio: float = 1, beam_size: int = 10) -> None:\n        super().__init__(vocab, encoder, decoder, max_timesteps, teacher_forcing, scheduled_sampling_ratio, beam_size)\n        \n    def _init_hidden(self, encoder: torch.Tensor) -> torch.Tensor:\n        mean_encoder = encoder.mean(dim=1)\n        \n        # Shape: (batch_size, decoder_dim)\n        initial_h = self._init_h(mean_encoder)\n\n        return initial_h\n    \n    def _decode(self, state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        # Get data from state\n        metadata = state['metadata']\n        x = state['x']\n        h = state['h']\n        label = state['label']\n        mask = state['mask']\n        \n        # Get actual size of current batch\n        local_batch_size = x.shape[0]\n\n        # Sort data to be able to only compute relevent parts of the batch at each timestep\n        # Shape: (batch_size)\n        lengths = mask.sum(dim=1)\n        # Shape: (batch_size) (batch_size)\n        sorted_lengths, indices = lengths.sort(dim=0, descending=True)\n        # Computing last timestep isn't necessary with labels since last timestep is eos token or pad token \n        timesteps = sorted_lengths[0] - 1\n\n        # Shape: (batch_size, ?)\n        # Shape: (batch_size, height * width, encoder_dim)\n        # Shape: (batch_size, decoder_dim)\n        # Shape: (batch_size, timesteps)\n        # Shape: (batch_size, timesteps)\n        metadata = [metadata[i] for i in indices]\n        x = x[indices]\n        h = h[indices]\n        label = label[indices]        \n        mask = mask[indices]\n        \n        # Shape: (batch_size, 1)\n        predicted_indices = torch.LongTensor([[self._start_index]] * local_batch_size).to(device).view(-1, 1)\n        \n        # Shape: (batch_size, timesteps, vocab_size)\n        predictions = torch.zeros(local_batch_size, timesteps, self._decoder.get_output_dim(), device=device)\n        attention_weights = torch.zeros(local_batch_size, timesteps, self._encoder.get_feature_map_size(), device=device)\n        sum_attention_weights = torch.zeros(local_batch_size, self._encoder.get_feature_map_size(), device=device)\n\n        for t in range(timesteps):\n            # Shape: (batch_offset)\n            batch_offset = sum([l > t for l in sorted_lengths.tolist()])\n\n            # Only compute data in valid timesteps\n            # Shape: (batch_offset, height * width, encoder_dim)\n            # Shape: (batch_offset, decoder_dim)\n            # Shape: (batch_offset, 1)\n            x_t = x[:batch_offset]\n            h_t = h[:batch_offset]\n            predicted_indices_t = predicted_indices[:batch_offset]\n            \n            # Decode timestep\n            # Shape: (batch_size, decoder_dim) (batch_size, vocab_size), (batch_size, encoder_dim, 1), (batch_size, height * width)\n            h, preds, attention_weight, sum_attention_weights = self._decoder(x_t, h_t, predicted_indices_t, sum_attention_weights)\n            \n            # Get new predicted indices to pass into model at next timestep\n            # Use teacher forcing if chosen\n            if self._teacher_forcing:\n                # Send next timestep's label to next timestep\n                # Shape: (batch_size, 1)\n                predicted_indices = label[:batch_offset, t + 1].view(-1, 1)\n            else:\n                # Shape: (batch_size, 1)\n                predicted_indices = torch.argmax(preds, dim=1).view(-1, 1)\n            \n            # Save preds\n            predictions[:batch_offset, t, :] = preds\n            attention_weights[:batch_offset, t, :] = attention_weight.view(-1, self._encoder.get_feature_map_size())\n            \n        # Update state and add logits\n        state['metadata'] = metadata\n        state['x'] = x\n        state['h'] = h\n        state['label'] = label\n        state['mask'] = mask\n        state['attention_weights'] = attention_weights\n        state['logits'] = predictions\n            \n        return state\n    \n    def _beam_search_step(self, last_predictions: torch.Tensor, state: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        # Group_size is batch_size * beam_size except for first decoding timestep where it is batch_size\n        # Shape: (group_size, decoder_dim) (group_size, vocab_size) (?) (group_size, height * width)\n        h, predictions, attention_weights, sum_attention_weights = self._decoder(state['x'], state['h'], last_predictions, state['sum_attention_weights'])\n        \n        if self._attention_weights is not None:\n            attention_weights = attention_weights.view(-1, self._beam_size, 1, self._encoder.get_feature_map_size())\n            self._attention_weights = torch.cat([self._attention_weights, attention_weights[:, 0, :, :]], dim=1)\n        else:\n            attention_weights = attention_weights.view(-1, 1, self._encoder.get_feature_map_size())\n            self._attention_weights = attention_weights\n\n        # Update state\n        # Shape: (group_size, decoder_dim)\n        # Shape: (group_size, height * width)\n        state['h'] = h\n        state['sum_attention_weights'] = sum_attention_weights\n\n        # Run log_softmax over logit predictions\n        # Shape: (group_size, vocab_size)\n        log_preds = F.log_softmax(predictions, dim=1)\n\n        return log_preds, state\n    \n    def _beam_search(self, state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        # Get data from state\n        x = state['x']\n        h = state['h']\n        \n        # Get actual size of current batch\n        local_batch_size = x.shape[0]\n\n        # Beam search wants initial preds of shape: (batch_size)\n        # Shape: (batch_size)\n        # Shape: (batch_size, height * width)    \n        initial_indices = torch.LongTensor([[self._start_index]] * local_batch_size).to(device).view(-1)\n        sum_attention_weights = torch.zeros(local_batch_size, self._encoder.get_feature_map_size(), device=device)\n\n        state = {'x': x, 'h': h, 'sum_attention_weights': sum_attention_weights}\n        \n        self._attention_weights = None\n\n        # Timesteps returned aren't necessarily max_timesteps\n        # Shape: (batch_size, beam_size, timesteps), (batch_size, beam_size)        \n        predictions, log_probabilities = self.beam_search.search(initial_indices, state, self._beam_search_step)\n\n        # Only keep best predictions from beam search\n        # Shape: (batch_size, timesteps)\n        predictions = predictions[:, 0, :].view(local_batch_size, -1)\n        \n        return predictions\n        \n    @overrides\n    def forward(self, metadata: object, img: torch.Tensor, label: Dict[str, torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n        # Encode the image\n        # Shape: (batch_size, height * width, encoder_dim)\n        x = self._encoder(img)\n\n        state = {'metadata': metadata, 'x': x}\n        # Compute loss on train and val\n        if label is not None:\n            # Initialize h and c\n            # Shape: (batch_size, decoder_dim)\n            state['h'] = self._init_hidden(x)\n\n            # Convert label dict to tensor since label isn't an input to the model and get mask\n            # Shape: (batch_size, timesteps)\n            state['mask'] = get_text_field_mask(label).to(device)\n            # Shape: (batch_size, timesteps)\n            state['label'] = label['tokens']\n\n            # Decode encoded image and get loss on train and val\n            state = self._decode(state)\n\n            # Loss shouldn't be computed on start token\n            state['mask'] = state['mask'][:, 1:].contiguous()\n            state['target'] = state['label'][:, 1:].contiguous()\n\n            # Compute cross entropy loss\n            state['loss'] = sequence_cross_entropy_with_logits(state['logits'], state['target'], state['mask'])\n            # No doubly stochastic loss in WAP\n            # Doubly stochastic regularization\n#             state['loss'] += ((1 - torch.sum(state['attention_weights'], dim=1)) ** 2).mean()\n\n        # Decode encoded image with beam search on val and test\n        if not self.training:\n            # (Re)initialize h\n            state['h'] = self._init_hidden(state['x'])\n            \n            # Run beam search\n            state['out'] = self._beam_search(state)\n\n            # Save attention weights\n            state['attention_weights'] = self._attention_weights\n\n            # Compute validation scores\n            if 'label' in state:\n                self._bleu(state['out'], state['target'])\n                self._exprate(state['out'], state['target'])\n            \n        # Set out to logits while training\n        else:\n            state['out'] = state['logits']\n            \n        return state\n    \n@Model.register('multiscale')\nclass Multiscale(ImageCaptioning):\n    def __init__(self, vocab: Vocabulary, encoder: Encoder, decoder: CaptioningDecoder, max_timesteps: int = 75, teacher_forcing: bool = True, scheduled_sampling_ratio: float = 1, beam_size: int = 10) -> None:\n        super().__init__(vocab, encoder, decoder, max_timesteps, teacher_forcing, scheduled_sampling_ratio, beam_size)\n\n    @overrides\n    def _init_hidden(self, encoder: torch.Tensor) -> torch.Tensor:\n        mean_encoder = encoder[0].mean(dim=1)\n        \n        # Shape: (batch_size, decoder_dim)\n        initial_h = self._init_h(mean_encoder)\n\n        return initial_h\n\n    @overrides\n    def _decode(self, state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        # Get data from state\n        metadata = state['metadata']\n        x = state['x']\n        h = state['h']\n        label = state['label']\n        mask = state['mask']\n        \n        # Get actual size of current batch\n        # Use main features to find current batch_size\n        local_batch_size = x[0].shape[0]\n\n        # Sort data to be able to only compute relevent parts of the batch at each timestep\n        # Shape: (batch_size)\n        lengths = mask.sum(dim=1)\n        # Shape: (batch_size) (batch_size)\n        sorted_lengths, indices = lengths.sort(dim=0, descending=True)\n        # Computing last timestep isn't necessary with labels since last timestep is eos token or pad token \n        timesteps = sorted_lengths[0] - 1\n\n        # Shape: (batch_size, ?)\n        # x is a list; Shape: (batch_size, height * width, encoder_dim), (batch_size, height * width, encoder_dim)\n        # Shape: (batch_size, decoder_dim)\n        # Shape: (batch_size, timesteps)\n        # Shape: (batch_size, timesteps)\n        metadata = [metadata[i] for i in indices]\n        # Sort indices of values in list separately\n        x = [x[0][indices], x[1][indices]]\n        h = h[indices]\n        label = label[indices]        \n        mask = mask[indices]\n        \n        # Shape: (batch_size, 1)\n        predicted_indices = torch.LongTensor([[self._start_index]] * local_batch_size).to(device).view(-1, 1)\n        \n        # Shape: (batch_size, timesteps, vocab_size)\n        predictions = torch.zeros(local_batch_size, timesteps, self._decoder.get_output_dim(), device=device)\n        # Attention weights is a tuple\n        attention_weights = (torch.zeros(local_batch_size, timesteps, self._encoder.get_feature_map_size(), device=device), torch.zeros(local_batch_size, timesteps, self._encoder.get_feature_map_size() * 2 * 2, device=device))\n        sum_attention_weights_0 = torch.zeros(local_batch_size, self._encoder.get_feature_map_size(), device=device)\n        sum_attention_weights_1 = torch.zeros(local_batch_size, self._encoder.get_feature_map_size() * 2 * 2, device=device)\n\n        for t in range(timesteps):\n            # Shape: (batch_offset)\n            batch_offset = sum([l > t for l in sorted_lengths.tolist()])\n\n            # Only compute data in valid timesteps\n            # x_t is a list; Shape: (batch_offset, height * width, encoder_dim), (batch_offset, height * width, encoder_dim)\n            # Shape: (batch_offset, decoder_dim)\n            # Shape: (batch_offset, decoder_dim)\n            # Shape: (batch_offset, 1)\n            x_t = [x[0][:batch_offset], x[1][:batch_offset]]\n            h_t = h[:batch_offset]\n            predicted_indices_t = predicted_indices[:batch_offset]\n            \n            # Decode timestep\n            # Shape: (batch_size, decoder_dim) (batch_size, vocab_size), (batch_size, encoder_dim, 1), (batch_size, height * width)\n            h, preds, attention_weight, sum_attention_weights_0, sum_attention_weights_1 = self._decoder(x_t, h_t, predicted_indices_t, sum_attention_weights_0, sum_attention_weights_1)\n            \n            # Get new predicted indices to pass into model at next timestep\n            # Use teacher forcing if chosen\n            if self._teacher_forcing and np.random.random() < self._scheduled_sampling_ratio:\n                # Send next timestep's label to next timestep\n                # Shape: (batch_size, 1)\n                predicted_indices = label[:batch_offset, t + 1].view(-1, 1)\n            else:\n                # Shape: (batch_size, 1)\n                predicted_indices = torch.argmax(preds, dim=1).view(-1, 1)\n            \n            # Save preds\n            predictions[:batch_offset, t, :] = preds\n            \n            # Attention weights is a tuple\n            attention_weights[0][:batch_offset, t, :] = attention_weight[0].view(-1, self._encoder.get_feature_map_size())\n            attention_weights[1][:batch_offset, t, :] = attention_weight[1].view(-1, self._encoder.get_feature_map_size() * 2 * 2)\n\n        # Update state and add logits\n        state['metadata'] = metadata\n        state['x'] = x\n        state['h'] = h\n        state['label'] = label\n        state['mask'] = mask\n        state['attention_weights'] = attention_weights\n        state['logits'] = predictions\n            \n        return state\n\n    def _beam_search_step(self, last_predictions: torch.Tensor, state: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        # Group_size is batch_size * beam_size except for first decoding timestep where it is batch_size\n        # Shape: (group_size, decoder_dim) (group_size, decoder_dim) (group_size, vocab_size)\n        \n        # Combine main and multiscale features\n        x = [state['x_0'], state['x_1']]\n        h, predictions, attention_weights, sum_attention_weights_0, sum_attention_weights_1 = self._decoder(x, state['h'], last_predictions, state['sum_attention_weights_0'], state['sum_attention_weights_1'])\n    \n        # Attention weights is a tuple with main and multiscale features\n        if self._attention_weights is not None:\n            attention_weights = (attention_weights[0].view(-1, self._beam_size, 1, self._encoder.get_feature_map_size()), attention_weights[1].view(-1, self._beam_size, 1, self._encoder.get_feature_map_size() * 2 * 2))\n            self._attention_weights = (torch.cat([self._attention_weights[0], attention_weights[0][:, 0, :, :]], dim=1), torch.cat([self._attention_weights[1], attention_weights[1][:, 0, :, :]], dim=1))\n        else:\n            attention_weights = (attention_weights[0].view(-1, 1, self._encoder.get_feature_map_size()), attention_weights[1].view(-1, 1, self._encoder.get_feature_map_size() * 2 * 2))\n            self._attention_weights = attention_weights\n\n        # Update state\n        # Shape: (group_size, decoder_dim)\n        state['h'] = h\n        \n        state['sum_attention_weights_0'] = sum_attention_weights_0\n        state['sum_attention_weights_0'] = sum_attention_weights_0\n\n        # Run log_softmax over logit predictions\n        # Shape: (group_size, vocab_size)\n        log_preds = F.log_softmax(predictions, dim=1)\n        state\n\n        return log_preds, state\n\n    @overrides\n    def _beam_search(self, state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        # Get data from state\n        x = state['x']\n        h = state['h']\n        \n        # x is a list; use main features; Get actual size of current batch\n        local_batch_size = x[0].shape[0]\n\n        # Beam search wants initial preds of shape: (batch_size)\n        # Shape: (batch_size)\n        initial_indices = torch.LongTensor([[self._start_index]] * local_batch_size).to(device).view(-1)        \n        sum_attention_weights_0 = torch.zeros(local_batch_size, self._encoder.get_feature_map_size(), device=device)\n        sum_attention_weights_1 = torch.zeros(local_batch_size, self._encoder.get_feature_map_size() * 2 * 2, device=device)\n\n        # Beam search requires tensors, not lists\n        state = {'x_0': x[0], 'x_1': x[1], 'h': h, 'sum_attention_weights_0': sum_attention_weights_0, 'sum_attention_weights_1': sum_attention_weights_1}\n        \n        # Timesteps returned aren't necessarily max_timesteps\n        # Shape: (batch_size, beam_size, timesteps), (batch_size, beam_size)\n        \n        self._attention_weights = None\n        \n        predictions, log_probabilities = self.beam_search.search(initial_indices, state, self._beam_search_step)\n\n        # Only keep best predictions from beam search\n        # Shape: (batch_size, timesteps)\n        predictions = predictions[:, 0, :].view(local_batch_size, -1)\n        \n        return predictions\n\n    @overrides\n    def forward(self, metadata: object, img: torch.Tensor, label: Dict[str, torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n        # Encode the image\n        # Shape: (batch_size, height * width, encoder_dim)\n        x = self._encoder(img)\n\n        state = {'metadata': metadata, 'x': x}\n        # Compute loss on train and val\n        if label is not None:\n            # Initialize h and c\n            # Shape: (batch_size, decoder_dim)\n            state['h'] = self._init_hidden(x)\n\n            # Convert label dict to tensor since label isn't an input to the model and get mask\n            # Shape: (batch_size, timesteps)\n            state['mask'] = get_text_field_mask(label).to(device)\n            # Shape: (batch_size, timesteps)\n            state['label'] = label['tokens']\n\n            # Decode encoded image and get loss on train and val\n            state = self._decode(state)\n\n            # Loss shouldn't be computed on start token\n            state['mask'] = state['mask'][:, 1:].contiguous()\n            state['target'] = state['label'][:, 1:].contiguous()\n\n            # Compute cross entropy loss\n            state['loss'] = sequence_cross_entropy_with_logits(state['logits'], state['target'], state['mask'])\n            # Doubly stochastic regularization\n            # Can't use doubly stochastic regularization with multiscale features\n            # state['loss'] += ((1 - torch.sum(state['attention_weights'], dim=1)) ** 2).mean()\n\n        # Decode encoded image with beam search on val and test\n        if not self.training:\n            # (Re)initialize h\n            state['h'] = self._init_hidden(state['x'])\n            \n            # Run beam search\n            state['out'] = self._beam_search(state)\n\n            # Save attention weights\n            # Predictor needs tensors, not tuple\n            state['main_attention_weights'] = self._attention_weights[0]\n            state['multiscale_attention_weights'] = self._attention_weights[1]\n\n            # Compute validation scores\n            if 'label' in state:\n                self._bleu(state['out'], state['target'])\n                self._exprate(state['out'], state['target'])\n            \n        # Set out to logits while training\n        else:\n            state['out'] = state['logits']\n            \n        return state","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab9b04b60a2e0897abb857ae5bdab657881ef017","scrolled":true},"cell_type":"code","source":"%%writefile math_handwriting_recognition/predictor.py\nimport os\nimport random\nfrom typing import Dict, Tuple\nfrom overrides import overrides\nimport json\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport skimage\nimport cv2\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchvision\n\nimport allennlp\n\nfrom allennlp.common import Registrable, Params\nfrom allennlp.common.util import START_SYMBOL, END_SYMBOL, JsonDict\n\nfrom allennlp.data import DatasetReader\nfrom allennlp.data.vocabulary import Vocabulary\n\nfrom allennlp.models import Model\n\nfrom allennlp.predictors.predictor import Predictor\n\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper # MIGHT USE FOR ABSTRACTION\n\nfrom allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\nfrom allennlp.nn.beam_search import BeamSearch\n\nfrom allennlp.training.metrics import F1Measure, BLEU\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n@Predictor.register('CROHME')\nclass MathPredictor(Predictor):\n    def __init__(self, model: Model, dataset_reader: DatasetReader) -> None:\n        super().__init__(model, dataset_reader)\n        \n        self._start_idx = np.random.randint(0, 100)\n        self._counter = self._start_idx\n\n    def dump_line(self, outputs: JsonDict) -> str:\n        beam_search_preds = [self._model.vocab.get_token_from_index(i) for i in outputs['out']]\n        preds = ' '.join(beam_search_preds)\n        idx = preds.index('@end@')\n        preds = preds[:idx]\n#         out = '\\n\\nPred: ' + preds + '\\n'\n        out = preds + '\\n'\n\n        if 'label' in outputs:\n            label = ' '.join([self._model.vocab.get_token_from_index(i) for i in outputs['label']])\n            end_idx = label.index('@end@')\n            label = label[8:end_idx]\n#             out += 'Gold: ' + label + '\\n'\n            out += label + '\\n'\n\n#         if 'logits' in outputs:\n#             logits = np.array(outputs['logits'])\n#             out += 'Logits: ' + str([self._model.vocab.get_token_from_index(np.argmax(logits[i])) for i in range(logits.shape[0])])\n\n        # Save visualizations for first 10 preds\n        if self._counter - self._start_idx < 10:\n            img = plt.imread(outputs['metadata']['path'])\n            img = cv2.resize(img, (512, 128))\n            \n            attention_weights = np.array(outputs['attention_weights'])\n            timesteps = attention_weights.shape[0]\n            \n            fig=plt.figure(figsize=(20, 20))\n            fig.tight_layout() \n            columns = 8\n            rows = 10\n            for i in range(1, timesteps + 1):\n                ax = fig.add_subplot(rows, columns, i)\n                ax.set_title(f'{beam_search_preds[i-1]}')\n\n                plt.imshow(img)\n\n                attention_weight = attention_weights[i-1].reshape(4, 16)\n#                 attention_weight = attention_weights[i-1].reshape(8, 32)\n                attention_weight = skimage.transform.pyramid_expand(attention_weight, upscale=32, sigma=8)\n#                 attention_weight = skimage.transform.pyramid_expand(attention_weight, upscale=16, sigma=8)\n                plt.imshow(attention_weight, alpha=0.8)\n            \n            save_path = 'visualization_' + outputs['metadata']['path'].split('/')[2] + f'_{self._counter}.png'\n            fig.savefig(save_path)\n            \n            self._counter += 1\n            \n        return out\n\n@Predictor.register('WAP')\nclass WAPPredictor(Predictor):\n    def __init__(self, model: Model, dataset_reader: DatasetReader) -> None:\n        super().__init__(model, dataset_reader)\n        \n        self._start_idx = np.random.randint(0, 100)\n        self._counter = self._start_idx\n\n    def dump_line(self, outputs: JsonDict) -> str:\n        beam_search_preds = [self._model.vocab.get_token_from_index(i) for i in outputs['out']]\n        out = '\\n\\nPred: ' + ' '.join(beam_search_preds) + '\\n'\n\n        if 'logits' in outputs:\n            logits = np.array(outputs['logits'])\n            out += 'Logits: ' + str([self._model.vocab.get_token_from_index(np.argmax(logits[i])) for i in range(logits.shape[0])]) + '\\n'\n                \n        if 'label' in outputs:\n            out += 'Gold: ' + ' '.join([self._model.vocab.get_token_from_index(i) for i in outputs['label']])\n                \n        # Save visualizations for first 10 preds\n        if self._counter - self._start_idx < 10:\n            img = plt.imread(outputs['metadata']['path'])\n            img = cv2.resize(img, (512, 128))\n            \n            attention_weights = np.array(outputs['attention_weights'])\n            timesteps = attention_weights.shape[0]\n\n            fig=plt.figure(figsize=(20, 20))\n            fig.tight_layout() \n            columns = 8\n            rows = 10\n            for i in range(1, timesteps + 1):\n                ax = fig.add_subplot(rows, columns, i)\n                ax.set_title(f'{beam_search_preds[i-1]}')\n                \n                plt.imshow(img)\n                \n                attention_weight = attention_weights[i-1].reshape(8, 32)\n                attention_weight = skimage.transform.pyramid_expand(attention_weight, upscale=16, sigma=8)\n                plt.imshow(attention_weight, alpha=0.8)\n                \n            save_path = 'visualization_' + outputs['metadata']['path'].split('/')[2] + f'_{self._counter}.png'\n            fig.savefig(save_path)\n            \n            self._counter += 1\n            \n        return out\n\n@Predictor.register('multiscale')\nclass MathPredictor(Predictor):\n    def __init__(self, model: Model, dataset_reader: DatasetReader) -> None:\n        super().__init__(model, dataset_reader)\n        \n        self._start_idx = np.random.randint(0, 100)\n        self._counter = self._start_idx\n\n    def dump_line(self, outputs: JsonDict) -> str:\n        beam_search_preds = [self._model.vocab.get_token_from_index(i) for i in outputs['out']]\n        out = '\\n\\nPred: ' + ' '.join(beam_search_preds) + '\\n'\n\n        if 'logits' in outputs:\n            logits = np.array(outputs['logits'])\n            out += 'Logits: ' + str([self._model.vocab.get_token_from_index(np.argmax(logits[i])) for i in range(logits.shape[0])]) + '\\n'\n                \n        if 'label' in outputs:\n            out += 'Gold: ' + ' '.join([self._model.vocab.get_token_from_index(i) for i in outputs['label']])\n    \n        # Save visualizations for first 10 preds\n        if self._counter - self._start_idx < 10:\n            img = plt.imread(outputs['metadata']['path'])\n            img = cv2.resize(img, (512, 128))\n            \n            attention_weights = np.array(outputs['main_attention_weights'])\n            timesteps = attention_weights.shape[0]\n\n            fig=plt.figure(figsize=(20, 20))\n            fig.tight_layout() \n            columns = 8\n            rows = 10\n            for i in range(1, timesteps + 1):\n                ax = fig.add_subplot(rows, columns, i)\n                ax.set_title(f'{beam_search_preds[i-1]}')\n                \n                plt.imshow(img)\n                \n                attention_weight = attention_weights[i-1].reshape(8, 32)\n                attention_weight = skimage.transform.pyramid_expand(attention_weight, upscale=16, sigma=8)\n                plt.imshow(attention_weight, alpha=0.8)\n                \n            save_path = 'visualization_' + outputs['metadata']['path'].split('/')[2] + f'_{self._counter}_main_branch.png'\n            fig.savefig(save_path)\n\n            attention_weights = np.array(outputs['multiscale_attention_weights'])\n            timesteps = attention_weights.shape[0]\n\n            fig=plt.figure(figsize=(20, 20))\n            fig.tight_layout() \n            columns = 8\n            rows = 10\n            for i in range(1, timesteps + 1):\n                ax = fig.add_subplot(rows, columns, i)\n                ax.set_title(f'{beam_search_preds[i-1]}')\n                \n                plt.imshow(img)\n                \n                attention_weight = attention_weights[i-1].reshape(16, 64)\n                attention_weight = skimage.transform.pyramid_expand(attention_weight, upscale=8, sigma=8)\n                plt.imshow(attention_weight, alpha=0.8)\n                \n            save_path = 'visualization_' + outputs['metadata']['path'].split('/')[2] + f'_{self._counter}_multiscale_branch.png'\n            fig.savefig(save_path)\n            \n            self._counter += 1\n            \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c6af1d90297231b7e9ad59cf671cfb407f48623"},"cell_type":"code","source":"%%writefile config.json\n{\n    \"dataset_reader\": {\n        \"type\": \"CROHME\",\n        \"root_path\": \"./\",\n        \"height\": 128,\n        \"width\": 512,\n        \"lazy\": true,\n        \"subset\": false,\n        \"tokenizer\": {\n            \"type\": \"latex\"\n        }\n    },\n    \"train_data_path\": \"crohme-train/train.csv\",\n    \"validation_data_path\": \"crohme-train/val.csv\",\n    \"model\": {\n        \"type\": \"image-captioning\",\n        \"encoder\": {\n            \"type\": \"lstm\",\n            \"encoder\": {\n                \"type\": 'backbone',\n                \"encoder_type\": 'resnet18',\n                \"encoder_height\": 4,\n                \"encoder_width\": 16,\n                \"pretrained\": true,\n                \"custom_in_conv\": false\n            },\n            \"layers\": 1,\n            \"bidirectional\": false\n        },\n        \"decoder\": {\n            \"type\": \"image-captioning\",\n            \"attention\": {\n                \"type\": 'image-captioning',\n                \"encoder_dim\": 512, # Must be encoder dim of chosen encoder\n                \"decoder_dim\": 256, # Must be same as decoder's decoder_dim\n                \"attention_dim\": 256,\n                \"doubly_stochastic_attention\": true\n            },\n            \"embedding_dim\": 256,\n            \"decoder_dim\": 256\n        },\n        \"max_timesteps\": 75,\n        \"beam_size\": 10,\n        \"teacher_forcing\": true,\n        \"scheduled_sampling_ratio\": 1,\n    },\n    \"iterator\": {\n        \"type\": \"bucket\",\n        \"sorting_keys\":[[\"label\", \"num_tokens\"]],\n        \"batch_size\": 16\n    },\n    \"trainer\": {\n        \"num_epochs\": 20,\n        \"cuda_device\": 0,\n        \"optimizer\": {\n            \"type\": \"sgd\",\n            \"lr\": 0.1,\n            \"momentum\": 0.9\n        },\n        \"grad_clipping\": 5,\n        \"validation_metric\": \"+exprate\",\n        \"learning_rate_scheduler\": {\n            \"type\": \"reduce_on_plateau\",\n            \"factor\": 0.5,\n            \"patience\": 5\n        },\n        \"num_serialized_models_to_keep\": 1,\n        \"summary_interval\": 10,\n        \"histogram_interval\": 100,\n        \"should_log_parameter_statistics\": true,\n        \"should_log_learning_rate\": true\n    }\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1daca70047cd681a89abbccaa87e91c55cf4917c"},"cell_type":"code","source":"# # Save vocabulary in advance\n# !allennlp make-vocab -s ./ --include-package math_handwriting_recognition config.json\n\n# # Find best learning rate\n# !allennlp find-lr -s ./logs --start-lr 0.001 --end-lr 10 --num-batches=100 --include-package math_handwriting_recognition config.json\n# x = plt.imread('./logs/lr-losses.png')\n# fig, ax = plt.subplots(figsize=(10, 10))\n# ax.imshow(x, interpolation='nearest')\n# !rm -rf logs/*\n\n# # Use Allennlp's online configuration tool\n# get_ipython().system_raw('allennlp configure --include-package math_handwriting_recognition &')\n# !ssh -o \"StrictHostKeyChecking no\" -R 80:localhost:8123 serveo.net\n\n# # Dry run configuration\n# !allennlp dry-run -s ./logs --include-package math_handwriting_recognition config.json\n# !rm -rf ./logs/*\n\n# # Predict with last checkpoint\n# !allennlp predict --output-file ./out.txt --weights-file ./logs/model_state_epoch_9.th --batch-size 64 --silent --cuda-device 0 --use-dataset-reader --predictor math-predictor --include-package math_handwriting_recognition ./logs/model.tar.gz test.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!allennlp train config.json -s ./logs --include-package math_handwriting_recognition\n# !rm -rf logs/*","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85518b69823ffe34757c8c0e8c50ed935d39f7fb","scrolled":true,"_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"!allennlp evaluate --cuda-device 0 --include-package math_handwriting_recognition ./logs/model.tar.gz crohme-train/train.csv\n!allennlp evaluate --cuda-device 0 --include-package math_handwriting_recognition ./logs/model.tar.gz crohme-train/val.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dc4ce8e63ec8957d1ef8b746339382ec3cb4ec3","scrolled":true,"_kg_hide-output":false},"cell_type":"code","source":"!allennlp predict --output-file ./out.txt --batch-size 64 --cuda-device 0 --use-dataset-reader --predictor CROHME --include-package math_handwriting_recognition --silent ./logs/model.tar.gz crohme-train/train.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!head -10 out.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65326548aaeb4ef9f4fe62e1b10cc0137ff2f80b","scrolled":true,"_kg_hide-output":true},"cell_type":"code","source":"!allennlp predict --output-file ./out.txt --batch-size 64 --cuda-device 0 --use-dataset-reader --predictor CROHME --include-package math_handwriting_recognition --silent ./logs/model.tar.gz crohme-train/val.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de5d62ee421e05e0407a91a483a725c92de4b87f","scrolled":false},"cell_type":"code","source":"!head -10 out.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"235d2a02faef841cbe0dc061b6996eccd711aad9"},"cell_type":"code","source":"from tensorboardX import SummaryWriter\n\nwriter = SummaryWriter('./logs')\n\nfor img_name in glob.glob('./logs/math-recognition/visualization_*.png'):\n    img = torch.from_numpy(plt.imread(img_name)[:, :, :3].transpose(2, 0, 1))\n\n    writer.add_image(img_name, img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17d0b21176461283a37c046472c911de09423f73","scrolled":false},"cell_type":"code","source":"!cat logs/metrics.json","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2133162a91d8ac05f488c18c3eb6fba9a708d23"},"cell_type":"code","source":"!rm -rf crohme-train\n!rm -rf crohme-val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e22317af4d8d515fa409c52b6eb62539c34081d"},"cell_type":"code","source":"with open('./logs/metrics.json', 'r') as metrics:\n    metrics = metrics.read()\n\nnotify('Metrics: ', f'{metrics}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56cfd019730c9e7fde357df02b0fd282d22959fe"},"cell_type":"code","source":"\n\n\n\n\n\n\n\n\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}